### KNN算法特点

- 优点
  - 精度高
  - 对异常值不敏感
  - 无数据输入假定
- 缺点
  - 计算复杂度高
  - 空间复杂度高
- 使用数据范围
  - 数值型和标称型



#### K值的选择

- 选择较小的K值
  - “学习”的近似误差(approximation error)会减小，但“学习”的估计误差(estimation error) 会增大
  - 对噪声敏感
  - K值的减小就意味着整体模型变得复杂，容易发生过拟合
- 选择较大的K值
  - 减少学习的估计误差，但缺点是学习的近似误差会增大
  - K值的增大就意味着整体的模型变得简单



### KNN面临的问题

- K值确定
  - 通常采用经验法则，且K值对准确度的影响非单调
- 特征选择
- 距离函数选择
  - Minkowski distance($L_p$ distance) or Mahalanobis distance?
- 复杂度
  - 需要计算和所有训练数据的距离，复杂度和训练数据大小成正比





###  马氏距离(Mahalanobis distance)

#### 引入

由P.C. Mahalanobis提出，是基于样本分布的一种距离测量。马氏距离考虑到各种特性之间的联系，可以消除样本间的相关性，广泛用于分类和聚类分析。

例如身高和体重，这两个变量拥有不同的单位标准，也就是有不同的scale。比如身高用毫米计算，而体重用千克计算，显然差10mm的身高与差10kg的体重是完全不同的。而且，如果简单地进行归一化处理不能解决样本分布对分类的影响。比如，在一个方差较小的维度下很小的差别就有可能成为离群点，下图中A与B相对于原点的距离是相同的。但是由于样本总体沿着横轴分布，所以B点更有可能是这个样本中的点，而A则更有可能是离群点。

![](https://pic4.zhimg.com/v2-6f5d1b59fd1687cfeecd0c6991c6db77_r.jpg)

还有一个问题——如果维度间不独立同分布，样本点一定与欧氏距离近的样本点同类的概率更大吗？

![](https://pic3.zhimg.com/v2-3cee35b79d272dda86e2604c160934ee_r.jpg)

可以看到样本基本服从 $f(x) = x$ 的线性分布，A与B相对于原点的距离依旧相等，显然A更像是一个离群点

即使数据已经经过了标准化，也不会改变AB与原点间距离大小的相互关系。所以要本质上解决这个问题，就要针对主成分分析中的主成分来进行标准化。

所以马氏距离的思路就是将变量按照主成分进行旋转，让维度间相互**独立**，然后进行标准化，让维度**同分布**。

由主成分分析可知，由于主成分就是特征向量方向，每个方向的方差就是对应的特征值，所以只需要按照特征向量的方向旋转，然后缩放特征值倍就可以了，可以得到以下的结果：

![](https://pic3.zhimg.com/80/v2-068306ff7e62b7af24b126eafe0b8bc6_720w.jpg)

#### 定义

一组向量 $\left\{\vec{X}_{1}, \vec{X}_{2}, \vec{X}_{3}, \ldots, \vec{X}_{n}\right\}$, 其中,
$$
\vec{X}=\left\{x_{1}, x_{2}, x_{3}, \ldots, x_{m}\right\}
$$
其均值为 $\vec{\mu}=\left\{\mu_{1}, \mu_{2}, \mu_{3}, \ldots, \mu_{m}\right\}$，协方差矩阵为 $\boldsymbol{\Sigma}$, 其中
$$
\Sigma_{i j}=\operatorname{cov}\left(x_{i}, x_{j}\right)
$$
单向量的马氏距离定义为:
$$
\operatorname{MD}(\vec{X})=\sqrt{(\vec{X}-\vec{\mu})^{T} \Sigma^{-1}(\vec{X}-\vec{\mu})}
$$
向量间的马氏距离定义为:
$$
\operatorname{MD}(\vec{X}, \vec{Y})=\sqrt{(\vec{X}-\vec{Y})^{T} \Sigma^{-1}(\vec{X}-\vec{Y})}
$$






### KD树

