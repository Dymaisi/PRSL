### 引入

决策树技术发现数据模式和规则的核心是归纳算法。与决策树相关的重要算法包括：CLS, ID3，C4.5，CART

- Hunt, Marin和Stone 于1966年研制的CLS学习系统，用于学习单个概念。
- 1979年, J.R. Quinlan 给出ID3算法，并在1983年和1986年对ID3 进行了总结和简化，使其成为决策树学习算法的典型。
- Schlimmer 和Fisher 于1986年对ID3进行改造，在每个可能的决策树节点创建缓冲区，使决策树可以递增式生成，得到ID4算法。
- 1988年，Utgoff 在ID4基础上提出了ID5学习算法，进一步提高了效率。
- 1993年，Quinlan 进一步发展了ID3算法，改进成C4.5算法。
- 另一类决策树算法为CART，与C4.5不同的是，CART的决策树由二元逻辑问题生成，每个树节点只有两个分枝，分别包括学习实例的正例与反例。

决策树内部节点表示一个特征或者属性，叶节点表示一个类。

#### 决策树与if-then规则

决策树可以看成一个 if-then 规则的集合，这很容易理解，决策树与对应的if-then集合是互斥且完备的，这意味着每一个实例都被一条路径或者一条规则覆盖，而且只被一条路径或者一条规则覆盖。

#### 决策树与条件概率分布

但决策树还可以表示给定特征条件下的条件概率分布，这一条件概率分布定义在特征空间的一个划分(partition)上。将特征空间划分为互不相交的单元(cell)或区域(region)，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。**决策树的一条路径对应于划分中的一个单元。**



### 决策树学习算法

包含特征选择、决策树的生成和剪枝三个过程。

### CLS算法

CLS(Concept Learning System)算法是早期的决策树学习算法。（不包含剪枝，实际上也不包含特征选择的准则）

CLS的基本思想：从一棵空决策树开始，选择某一属性（分类属性）作为测试属性。该测试属性对应决策树中的决策结点。根据该属性的值的不同，可将训练样本分成相应的子集：

- 如果该子集为空，或该子集中的样本属于同一个类，则该子集为叶结点。
- 否则该子集对应于决策树的内部结点，即测试结点，需要选择一个新的分类属性对该子集进行划分，直到所有的子集都为空或者属于同一类。

CLS的问题在于测试属性的选择时，没有规定采用何种测试属性。实践表明，测试属性集的组成以及测试属性的先后对决策树的学习具有举足轻重的影响。

### ID3算法

ID3算法是一种经典的决策树学习算法，由Quinlan于1979年提出。ID3算法主要针对属性选择问题。是决策树学习方法中最具影响和最为典型的算法。**该方法使用信息增益度选择测试属性**。





### CART算法